{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador Bayesiano\n",
    "\n",
    "Um problema de classificação pode ser apresentado como um modelo que computa probabilidades condicionais de uma classe dado uma amostra dos dados. O Teorema de Bayes (ou teorema Bayesiano) oferece um método para fazer esse calculo, embora na prática seja necessário uma quantidade muito grande de amostras, o que pode se tornar caro em termos computacionais.\n",
    "\n",
    "Em vez disso, o calculo do Teorema de Bayes pode ser simplificado assumindo algumas propriedades, como cada variável (característica) sendo considerada independente de todas as demais. Apesar de assumir um cenário dramático e irreal, tem o efeito de tornar o calculo da probabilidade condicional tratável e resulta em um modelo de classificação efetivo chamado Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação baseada em probabilidades condicionais\n",
    "\n",
    "Uma das abordagens para resolver problemas de classificação é através de modelos probabilísticos, cujo objetivo é estimar a probabilidade condicional da classe, dada a observação (amostra).\n",
    "\n",
    "Como exemplo, um prolema de classificação pode compreender $k$ rótulos $y_1, y_2\\dots y_k$ e $n$ características, $x_1, x_2,\\dots x_n$. Podemos calcular a probabilidade de predizer a classe de uma instância condicionada ao valor de cada coluna $x_1, x_2,\\dots x_n$ da seguinte maneira:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(y_i | x_1, x_2,\\dots, x_n)\n",
    "\\end{equation}\n",
    "\n",
    "A probabilidade condicional pode ser computada para cada classe e o rótulo com maior probabilidade pode ser retornado como a classificação mais provável.\n",
    "\n",
    "A probabilidade condicional pode ser calculada usando a probabilidade conjunta, embora seja intratável. O Teorema de Bayes oferece uma maneira simples de computar a probabilidade conjunta:\n",
    "\n",
    "<img src=\"assets/teoremaBayes.jpeg\" width=\"400\"/>\n",
    "\n",
    "Onde a probabilidade que estamos interessados em calcular $P(A|B)$ é chamada de probabilidade a posteriori e a probabilidade marginal de um evento $P(A)$ é chamada priori.\n",
    "\n",
    "Nota: O termo \"marginal\" vem da ideia de que são encontrados através dos valores em uma linha ou coluna, e seria colocano no final da linha ou coluna, ou seja, na margem. Como exemplo, $P(A) = 1$ é igual a soma (depois normalizada) de todas as amostras de classe $1$. Por isso é chamada a priori, podemos encontrar esse valor só de olhar os dados, no \"princípio\" do processo.\n",
    "\n",
    "Podemos apresentar o problema de classificação condicional usando o Teorema de Bayes da seguinte maneira:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(y_i | x_1, x_2,\\dots, x_n) = \\frac{P(x_1, x_2,\\dots, x_n| y_i) * P(y_i)}{P(x_1, x_2,\\dots, x_n)}\n",
    "\\end{equation}\n",
    "\n",
    "O valor a priori $P(y_i)$ é facil de estimar a partir do dataset, mas a probabilidade condicional da observação baseada na classe $P(x_1, x_2,\\dots, x_n| y_i)$ não é factível a não ser que o número de amostras seja extraordinariamente grande, ou seja, grande o suficiente para estimar a distribuição de probabilidades para todas as possíveis combinações de valores.\n",
    "\n",
    "Desse modo, uma aplicação direta do Teorema de Bayes também se torna intratável, especialmente quando o número de características das amostras ($n$) é grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplificação - Naive Bayes\n",
    "\n",
    "A solução para usar o Teorema de Bayes um modelo de classificação de probabilidade condicional é simplificar o cálculo. O Teorema de Bayes assume que cada variável (característica) é dependente de todas as outras, e é isso que causa a complexidade no cálculo. Podemos remover essa suposição e considerar cada variável como independente uma das outras.\n",
    "\n",
    "Isso muda o conceito de um modelo de probabilidade condicional dependente para um modelo de probabilidade condicional independente, simplificando dramáticamente os calculos.\n",
    "\n",
    "Primeiramente, o denominador é removido do calculo $P(x_1, x_2,\\dots, x_n)$ uma vez que esse termo é uma constante usada para calcular a probabilidade condicional de cada classe para uma determinada instância e tem o efeito de normalizar o resultado.\n",
    "\n",
    "\\begin{equation}\n",
    "    P(y_i | x_1, x_2,\\dots, x_n) = P(x_1, x_2,\\dots, x_n| y_i) * P(y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Em seguida, a probabilidade condicional de todas as variáveis dado o rótulo é mudada para diferentes probabilidades condicionais de cada variável dado o rótulo. Essas variáveis condicionais independentes são então multiplicadas juntas. Por exemplo:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(y_i | x_1, x_2,\\dots, x_n) = P(x_1|yi) * P(x2|yi) * \\dots * P(x_n|yi) * P(y_i)\n",
    "\\end{equation}\n",
    "\n",
    "Esse calculo pode ser executado para cada rótulo, e o rótulo com maior probabilidade pode ser selecionado como a classe da instância. Essa regra de decisão é conhecida como \"Máximo a posteriori\" ou MAP.\n",
    "\n",
    "Essa simplificação do Theorema de Bayes é comum e muito utilizada para problemas de classificação, conhecida como Naive Bayes. Note que a palavra \"naive\" vem do francês _naïve_, que quer dizer ingênuo, e \"Bayes\" vem do nome do criador do teorema, Thomas Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como computar a probabilidade condicional e a priori \n",
    "\n",
    "Calcular a probabilidade a priori $P(y_i)$ é simples. Pode ser estimado dividindo a frequência das observações no conjunto de treinamento com classe $y_i$ pelo total de amostras de treinamento:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(y_i) = \\frac{\\text{examples with } y_i}{\\text{ total examples}}\n",
    "\\end{equation}\n",
    "\n",
    "A probabilidade condicional de uma característica dado o rótulo também pode ser estimado a partir dos dados. Para isso, precisamos considerar o número de classes e uma distribuição dos dados por características. Em outras palavras, se temos $k$ classes e $n$ características, temos $k*n$ diferentes distribuições de probabilidades que precisarão ser criadas e armazenadas.\n",
    "\n",
    "Uma abordagem diferente pode ser necessária dependendo do tipo do dado de cada característica. Especificamente, os dados são usados para estimar os parâmetros de uma entre três distribuições de probabilidade padrão\n",
    "\n",
    "No caso de variáveis categóricas, como contagens ou rótulos, uma distribuição multinomial pode ser usada. Se as variáveis são binárias, podemos usar uma distribuição binomial. E se a variável for numérica usamos uma distribuição Gaussiana.\n",
    "\n",
    "- Binaria: distribuição binomial.\n",
    "- Categorica: distribuição multinomial.\n",
    "- Numerica: distribuição Gaussiana.\n",
    "\n",
    "Essas distribuições são tão comuns que as implementações do Naive Bayes geralmente são nomeadas em conjunto com a distribuição:\n",
    "\n",
    "\n",
    "- Binomial Naive Bayes: Naive Bayes que usa a distribuição binomial.\n",
    "- Multinomial Naive Bayes: Naive Bayes que usa a distribuição multinomial.\n",
    "- Gaussian Naive Bayes: Naive Bayes que usa a distribuição Gaussiana.\n",
    "\n",
    "Datasets com características de tipos mistos podem necessitar a seleção de diferentes tipos de distribuições de dados para cara característica.\n",
    "\n",
    "Usar uma das três distribuições comuns não é obrigatório; por exemplo, se soubermos que uma variável real pode ser modelada por uma distribuição diferente, como uma exponencial, podemos utilizar essa distribuição nesse caso. Se uma variável do tipo real não pertence a uma distribuição bem definida, como binomial ou multimodal, podemos usar um kernel estimador de densidade para escolher a melhor distribuição de probabilidade.\n",
    "\n",
    "\n",
    "O algoritmo Naive Bayes se mostrou eficiente e popular para tarefas de classificação de texto. As palavras em um documento podem ser codificadas como valores binário (se existe a palavra no texto), categóricas (contagem de aparições), ou frequência que aparecem no texto (valor real), sendo representadas por distribuições de probabilidades binárias, multinomiais, e Gaussianas, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando o Naive Bayes\n",
    "\n",
    "Vamos começar gerando 100 amostras com 2 características numéricas e pertencendo a 2 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n",
      "[[-0.79415228  2.10495117]\n",
      " [-9.15155186 -4.81286449]\n",
      " [-3.10367371  3.90202401]\n",
      " [-1.42946517  5.16850105]\n",
      " [-7.4693868  -4.20198333]]\n",
      "[0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.datasets import make_blobs\n",
    "# gerando um dataset de 2 dimensões para classificação\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# sumarizando\n",
    "print(X.shape, y.shape)\n",
    "print(X[:5])\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotando\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nossos vetores de características são representados por números reais, vamos utilizar uma distribuição Gaussiana de probabilidades .\n",
    "\n",
    "Podemos utilizar a API `norm` do pacote SciPy (norm porque a distribuição Gaussiana também é chamada de normal). Primeiramente precisamos encontrar os parâmetros da distribuição, i.e., média e desvio padrão, e então a função de probabilidade da densidade pode ser amostrada para um valor específico usando a função `norm.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajusta a distribuição de probabilidade para um conjunto de dados com uma única dimensão (univariado)\n",
    "def fit_distribution(data):\n",
    "    # estima os parâmetros\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data)\n",
    "    print(mu, sigma)\n",
    "    # ajusta a distribuição\n",
    "    dist = norm(mu, sigma)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que queremos computar a probabilidade condicional para cada característica do nosso vetor de características (por isso univariado). Isso significa que precisaremos de uma distribuição para cada característica, e um conjunto de distribuições para cada rótulo, somando 4 distribuições no total.\n",
    "\n",
    "Para isso, primeiro vamos dividir os dados por classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2) (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# separando os dados por classes\n",
    "Xy0 = X[y == 0]\n",
    "Xy1 = X[y == 1]\n",
    "print(Xy0.shape, Xy1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos então usar esses grupos para calcular as probabilidades a priori para as amostragens pertencentes a cada grupo.\n",
    "\n",
    "Essa probabilidade será de 50%, uma vez que criamos o dataset com o mesmo número de amostras para cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "# calculando as probabilidades a priori\n",
    "priory0 = len(Xy0) / len(X)\n",
    "priory1 = len(Xy1) / len(X)\n",
    "print(priory0, priory1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos chamar a função `fit_distribution()` que definimos para preparar a probabilidade de distribuição para cara característica e para cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5632888906409914 0.787444265443213\n",
      "4.426680361487157 0.958296071258367\n",
      "-9.681177100524485 0.8943078901048118\n",
      "-3.9713794295185845 0.9308177595208521\n"
     ]
    }
   ],
   "source": [
    "# cria as funções de probabilidade de densidade para y==0\n",
    "X1y0 = fit_distribution(Xy0[:, 0])\n",
    "X2y0 = fit_distribution(Xy0[:, 1])\n",
    "# cria as funções de probabilidade de densidade para y==1\n",
    "X1y1 = fit_distribution(Xy1[:, 0])\n",
    "X2y1 = fit_distribution(Xy1[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima são printadas as médias e desvios para cada uma das 4 combinações de classes e características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na sequência, vamos preparar o modelo probabilístico para fazer a predição.\n",
    "\n",
    "A probabilidade condicional independente para cada rótulo pode ser computado usando a probabilidade da classe a priori e a probabilidade condicional do valor de cada característica.\n",
    "\n",
    "A função de probabilidade a seguir executa esse cálculo para cada amostra (representada por um array de 2 características) dadas as probabilidades a priori e a distribuição de probabilidade condicional para cada variável. O valor retornado é uma pontuação em vez da probabilidade, uma vez que não está normalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(X, prior, dist1, dist2):\n",
    "    return prior * dist1.pdf(X[0]) * dist2.pdf(X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar essa função para calcular a probabilidade para uma amostra pertencente a cada classe.\n",
    "\n",
    "Primeiramente, vamos selecionar um exemplo a ser classificado; nesse exemplo, a primeira amostra do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classificando a primeira amostra do dataset\n",
    "Xsample, ysample = X[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, vamos computar a pontuação de uma amostra como pertencente a primeira classe, e depois como da segunda classe e mostrar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=0 | [-0.79415228  2.10495117]) = 0.348\n",
      "P(y=1 | [-0.79415228  2.10495117]) = 0.000\n"
     ]
    }
   ],
   "source": [
    "py0 = probability(Xsample, priory0, X1y0, X2y0)\n",
    "py1 = probability(Xsample, priory1, X1y1, X2y1)\n",
    "print('P(y=0 | %s) = %.3f' % (Xsample, py0*100))\n",
    "print('P(y=1 | %s) = %.3f' % (Xsample, py1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos ver as probabilidades em vez dessa pontuação, basta normalizarmos esses valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=0 | [-0.79415228  2.10495117]) = 1.000\n",
      "P(y=1 | [-0.79415228  2.10495117]) = 0.000\n",
      "Truth: y=0\n"
     ]
    }
   ],
   "source": [
    "denominator = py0*100 + py1*100\n",
    "\n",
    "print('P(y=0 | %s) = %.3f' % (Xsample, py0*100/denominator))\n",
    "print('P(y=1 | %s) = %.3f' % (Xsample, py1*100/denominator))\n",
    "\n",
    "# rotulo real\n",
    "print('Truth: y=%d' % ysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse caso, o algoritmo nos deu 100% de certeza de que a amostra é da classe 0.\n",
    "\n",
    "Na prática, é uma boa ideia usar uma versão otimizada do Naive Bayes, como as implementadas no scikit-learn. Lá temos 3 versões, BernoulliNB, MultinomialNB e GaussianNB, para as versões com distribuições binomial, multinomial e Gaussiana, respectivamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação no scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidade estimada:  [[1.00000000e+00 5.52387327e-30]]\n",
      "Classe predita: pred=0\n",
      "Original: y=0\n"
     ]
    }
   ],
   "source": [
    "# exemplo usando Naive Bayes Gaussiano\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# gera um dataset com 100 amostras, 2 classes e 2 dimensões\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
    "# define o modelo usando distribuição gaussiana\n",
    "model = GaussianNB()\n",
    "# ajusta o modelo\n",
    "model.fit(X, y)\n",
    "# escolhe uma única amostra\n",
    "Xsample, ysample = [X[0]], y[0]\n",
    "# faz uma estimativa da probabilidade\n",
    "yhat_prob = model.predict_proba(Xsample)\n",
    "print('Probabilidade estimada: ', yhat_prob)\n",
    "# faz a predição do rótulo\n",
    "yhat_class = model.predict(Xsample)\n",
    "print('Classe predita: pred=%d' % yhat_class[0])\n",
    "print('Original: y=%d' % ysample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quando usar Naive Bayes\n",
    "\n",
    "Uma vez que o classificador Bayesiano assume a não dependência entre os dados, no geral ele não é tão bom quanto outros classificadores mais complexos. Dito isso, podemos observar algumas vantagens:\n",
    "\n",
    "- É extremamente rápido tanto para treinamento quanto para inferência\n",
    "- Oferece uma predição de probabilidades\n",
    "- Facilmente interpretável\n",
    "- Poucos ou nenhum hyperparâmetro a ser otimizado\n",
    "\n",
    "Essas vantagens podem indicar que o classificador Naive Bayes pode ser frequentemente uma boa escolha como um _baseline_ para classificação. Se der bons resultados, temos um classificador bom, rápido e interpretável para o problema. Caso contrário, podemos explorar modelos mais complexos e podemos usá-lo para comparar os resultados.\n",
    "\n",
    "O Naive Bayes tende a funcionar especialmente bem para alguns casos:\n",
    "\n",
    "- Quando a suposição \"ingênua\" de fato se ajusta aos dados (muito raro na prática)\n",
    "- Para classes muito bem separadas, quando a complexidade não é muito importante\n",
    "- Para dados com dimensionalidade muito alta em que a complexidade não seja importante\n",
    "\n",
    "Os dois últimos pontos parecem diferentes, mas estão relacionados: quando a dimensionalidade do dataset aumenta, é muito menos provável que dois pontos sejam encontrados juntos (ou seja, eles precisariam estar próximos em TODAS as dimensões para estarem próximos de fato). Isso significa que agrupamentos em alta dimensionalidade tendem a ser mais separáveis, em média, que agrupamentos em baixa dimensionalidade, assumindo que as novas dimensões de fato agreguem informações importantes. Por isso, classificadores mais simplistas como o Naive Bayes tendem a funcionar tão bem ou melhor que alguns classificadores mais complexos conforme a dimensionalidade cresce: uma vez que tem dados suficiente, até mesmo modelos simples podem se tornar poderosos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício:\n",
    "\n",
    "1. pegar um dataset (ex: Iris, MNIST, Fashion MNIST) e rodar todos os algoritmos que vimos nas aulas (i.e., perceptron, MLP, SVM, Bayes, OPF, k-nn regressão logística) e comparar os resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
