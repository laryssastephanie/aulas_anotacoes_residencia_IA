{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao PyTorch\n",
    "\n",
    "\n",
    "[PyTorch](http://pytorch.org/) é um framework para desenolver e treinar redes neurais. Muitas de suas funções se comportam exatamente da mesma forma que o numpy, onde os arrays são chamados de tensores. A vantagem desses tensores em relação aos arrays do numpy é que eles facilitam a movimentação dos dados da CPU para a GPU, e também são usados por funções do PyTorch para computar altomaticamente gradientes (para o backpropagation) e outros modulos para construir redes neurais. No geral, PyTorch é mais coerente com programação Python e Numpy/Scipy quando comparada com TensorFlow ou outros frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos na Regressão Linear, Regressão Logística e Perceptron, uma coisa muito comum na área de aprendizado de máquinas é resolver equações lineares do tipo:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Em forma de vetores, podemos representar o produto escalar:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensores\n",
    "\n",
    "A grosso modo, algoritmos de aprendizado envolvem um monte de operações de algebra linear em tensores, uma generalização de matrizes. Um vetor é um tensor de 1 dimensão, e uma matriz é um tensor de 2 dimensões, e um array com 3 dimensões é um tensor tridimensional (para imagens com RGB, por exemplo). Sendo assim, a estrutura fundamental de redes neurais e PyTorch são os tensores.\n",
    "\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "Vejamos alguns exemplos de uso do PyTorch e seus tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiramente, importamos o PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Função de ativação - Sigmoid\n",
    "    \n",
    "        Argumentos\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gerando dados aleatórios\n",
    "torch.manual_seed(7) # Seta um seed para sempre gerar os mesmos números aleatórios\n",
    "\n",
    "# Features é um tensor com 1 linha (1 única amostra) e 5 colunas (5 características por amostra), \n",
    "#   inicializadas de forma aleatória usando uma distribuição normal com média zero e desvio 1.\n",
    "features = torch.randn((1, 5))\n",
    "\n",
    "# Gerando os pesos aleatórios para o nosso modelo. randn_like gera um tensor com as mesmas carácterísticas\n",
    "#    que o tensor passado como parâmetro\n",
    "weights = torch.randn_like(features)\n",
    "\n",
    "\n",
    "# termo de bias - tensor com uma única linha e coluna.\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses tensores podem ser somados, subtraidos, multiplicados, etc, assim como os arrays numpy. Em geral, usamos os tensores de forma bem parecida com esses arrays, com a vantagem de poder utilizar em GPUs. Como exemplo, podemos computar a saída desse nosso modelo de neurônio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos predizer a saída do nosso neurônio:\n",
    "\n",
    "# Assim como numpy, podemos usar as opções torch.sum(), assim como o métodos .sum() nos tensores.\n",
    "\n",
    "# opção 1 - torch.sum()\n",
    "y = activation(torch.sum(features * weights) + bias)\n",
    "print('opção 1: ', y)\n",
    "\n",
    "# opção 2 - .sum()\n",
    "y = activation((features * weights).sum() + bias)\n",
    "print('opção 2: ', y)\n",
    "\n",
    "# Podemos também juntar as operações de soma e multiplicação numa única operação, executando a multiplicação\n",
    "#   de matrizes. Em geral, multiplicação de matriz é mais eficiente, principalmente em GPUs. \n",
    "#   Para tanto, podemos utilizar as funções torch.mm() ou torch.matmul() - a última é mais complicada\n",
    "#      e oferece mais opções (verificar em https://pytorch.org/docs/stable/generated/torch.matmul.html).\n",
    "\n",
    "# opção 3 - torch.mm()\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)\n",
    "print('opção 3: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que na opção 3 tivemos que redimensionar nosso vetor de pesos com a função .view(). Caso contrário, teríamos um erro muito comum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(features, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso por que, na multiplicação de matrizes, o número de colunas no primeiro tensor deve ser igual ao número de linhas do segundo tensor. Tanto o tensor features quanto o tensor weights tem o mesmo formato, i.e., (1,5). Sendo assim, foi necessário mudar o formato para que a multiplicação funcionasse.\n",
    "\n",
    "Obs: para ver o formato de um tensor, podemos simplesmente usar .shape.\n",
    "\n",
    "Existem também algumas opções para mudar o formato do tensor: [`.reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape), [`.resize_()`](https://pytorch.org/docs/stable/generated/torch.Tensor.resize_.html#torch.Tensor.resize_) e [`.view()`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view).\n",
    "\n",
    "Usando como exemplo nosso tensor weights:\n",
    "\n",
    "* `weights.reshape(a, b)` retorna um novo tensor com os mesmos dados de `weights` com tamanho `(a, b)`, quando possível retorna apenas um view(), quando não, faz uma cópia dos dados.\n",
    "\n",
    "* `weights.resize_(a, b)` retorna o mesmo tensor com um formato diferente. Se o número de elementos for menor que o original, alguns elementos serão removidos do tensor (mas não da memória). Se o novo formato tem mais elementos, estes serão inicializados na memória. Note que o _ quer dizer que as operações são executadas **in-place**. [Clique aqui para mais informações a respeito](https://discuss.pytorch.org/t/what-is-in-place-operation/16244).\n",
    "\n",
    "* `weights.view(a, b)` retorna um tensor  no formato `(a, b)` com os mesmos dados contidos wm `weights`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy para Torch e vice-versa\n",
    "\n",
    "PyTorch apresenta diversos modos de converter entre arrays Numpy e tensores Torch. Vejamos alguns exemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando um array numpy de (4,3)\n",
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo de numpy para tensor\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertendo de tensor para numpy\n",
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A memória é compartilhada entre o array Numpy e o tensor Torch, então, se o valor de um objeto for mudado _in-place_ , o outro objeto também será alterado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicando o tensor por 2, in-place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O array numpy é ajustado ao novo valor\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão linear com PyTorch\n",
    "\n",
    "<img src=\"assets/rl.png\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo n°1: importando pacotes\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando a versão do PyTorch e se está usando GPU\n",
    "print('Versão PyTorch: ', torch.__version__)\n",
    "print('Usando GPU: ', torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    print('GPU: ',torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo n°2: gerando algumas amostras\n",
    "x_data = Variable(torch.Tensor([[10.0], [9.0], [3.0], [2.0]]))\n",
    "y_data = Variable(torch.Tensor([[90.0], [80.0], [50.0], [30.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para configurar a classe do nosso modelo, precisamos definir a classe init (onde definimos os atributos) e forward (em redes neurais é muito comum, visto que a etapa de atualização dos pesos é feita de modo backward com o backpropagation). \n",
    "\n",
    "Como o modelo recebe como entrada uma amostra com uma única _feature_ e entrega como saída um único valor, inicializamos o modelo com uma camada linear: torch.nn.Linear(1, 1). Linear por que essa camada executa uma combinação linear (os pesos e bias são intrínsicos na camada e não precisamos definir). O primeiro 1 representa o número de características de entrada, e o segundo 1 significa o tamanho da saída.\n",
    "\n",
    "Em seguida, definimos a função forward, que basicamente contém as instruções da sequência dos passos do modelo. Em outras palavras, esse passo executa todos os processos do modelo desde os dados de entrada até a saída. Como a regressão linear é bem simples, a função recebe uma entrada $x$ e produz uma estimativa de $y$ como saída, ou seja, $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Loss (Criterion) e Otimizador\n",
    "\n",
    "Após executar a função forward, a função de loss é usada para computar o quão distante está $\\hat{y}$ de $y$, e assim ajustar os pesos para aproximar essa diferença, a fim de produzir o melhor modelo possível. Definir essa função de loss no PyTorch é muito simples. Nesse caso usaremos o erro médio quadrado ( _Mean Square Error (MSE)_ ), por ser mais comum na tarefa de regressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na sequência, usaremos o otimizador Gradiente Descendente Estocástico (_Stochastic Gradient Descent (SGD)_) para atualizar os pesos do modelo. A função model.parameters() diz ao otimizador quais são os pesos a serem atualizados, enquanto _lr_ instrui qual a taxa de aprendizado será utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo\n",
    "\n",
    "Agora nosso modelo está pronto para ser treinado. O procedimento será executado por $20$ épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    \n",
    "    # Zera os gradientes a cada época (usado no backpropagation).\n",
    "    #   esse passo é necessário pois a cada vez que o erro é propagado, \n",
    "    #   ele é acumulado em vez de ser substituido.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    # Computa o erro\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    # Propaga o erro para as camadas anteriores \n",
    "    #    (no caso só temos um, mas seguimos o padrão de redes maiores)\n",
    "    loss.backward()\n",
    "    # Atualiza os pesos\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo predições\n",
    "\n",
    "Agora que o modelo está treinado, podemos usá-lo para fazer predições dado novos valores de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = Variable(torch.Tensor([[4.0]]))\n",
    "y_pred = model(new_x)\n",
    "print(\"Valor estimado: \", float(y_pred.data[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão Logística\n",
    "\n",
    "<img src=\"assets/regLog.png\" width=400px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando pacotes\n",
    "\n",
    "# https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19\n",
    "    \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o dataset\n",
    "\n",
    "Usamos torchvision.datasets para carregar o dataset Fashion-MNIST. Transforms são ferramentas de normalização, aumento de dados, entre outros. Nesse caso utilizaremos apenas para transformar em vetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando um data loader\n",
    "\n",
    "Conjuntos de dados grandes não podem ser carregados diretamente na memória, principalmente da GPU, por falta de espaço. Para isso, utilizamos data loaders, para carregar _batches_, ou seja, porções de amostras, a cada chamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo os hyperparâmetros e instanciando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 3000\n",
    "epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "lr_rate = 0.001\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciando a classe de Loss\n",
    "\n",
    "Em seguida definimos nossa função de loss, no caso a entropia cruzada (Cross-Entropy (CE)). Note que CE é praticamente a função Maximum Likelihood Estimation (MLE) que aprendemos na aula passada, com sinal inverso, ou seja, minimizar o BCE é praticamente a mesma coisa que maximizar o MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() # computes softmax and then the cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo o otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter+=1\n",
    "        if iter%500==0:\n",
    "            # calculate Accuracy\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 28*28))\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total+= labels.size(0)\n",
    "                # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "                correct+= (predicted == labels).sum()\n",
    "            accuracy = 100 * correct.float()/total\n",
    "            print(\"Iteration: {}. Loss: {}. Accuracy: {}.\".format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "1. Plotar algumas imagens do dataset fashion MNIST para ter uma idea de como são as amostras.\n",
    "2. Estimar os rótulos das amostras de teste, computar a acurácia e gerar uma matriz de confusão para ver com quais classes cada classe está se confundindo\n",
    "3. Rodar esse algoritmo utilizando o dataset Kuzushiji-MNIST (KMNIST). Pesquisar para fazer aumento de dados (transform) com horizontal flipping. Plotar algumas imagens desse dataset também. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
